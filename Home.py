import streamlit as st
import pandas as pd
import os
import json
from src.arxiv_search import search_arxiv, display_arxiv_results
from src.translator import translate
from src.utils import load_csv
from langchain_community.document_loaders import ArxivLoader


def load_arxiv(arxiv_id, with_meta=True):
    loader = ArxivLoader(arxiv_id, load_all_available_meta=with_meta)
    return loader.load()


def dataframe_with_selections(df):
    df_with_selections = df.copy()
    df_with_selections.insert(0, "Select", False)

    edited_df = st.data_editor(
        df_with_selections,
        hide_index=True,
        column_config={"Select": st.column_config.CheckboxColumn(required=True)},
        disabled=df.columns,
    )

    selected_rows = edited_df[edited_df.Select]

    return selected_rows.drop(columns=["Select"])


def save_paper_to_csv(arxiv_id, arxiv_data):
    if os.path.exists(f"./data/paper_csv/paper.csv"):
        df = pd.read_csv(f"./data/paper_csv/paper.csv")

        if arxiv_id in df["arxiv_id"].values:
            st.write("ì´ë¯¸ ì €ì¥ëœ ë…¼ë¬¸ì…ë‹ˆë‹¤.")
            return
        else:
            new_df = pd.json_normalize(arxiv_data[0].dict()["metadata"])
            new_df["arxiv_id"] = arxiv_id
            df = pd.concat([df, new_df])
            df.to_csv(f"./data/paper_csv/paper.csv", index=False)
    else:
        df = pd.json_normalize(arxiv_data[0].dict()["metadata"])
        df["arxiv_id"] = arxiv_id
        df.to_csv(f"./data/paper_csv/paper.csv", index=False)


def handle_arxiv_search(query):
    results = search_arxiv(query)
    summary_button = st.checkbox("Full Summary", False)

    if st.button("ê²€ìƒ‰"):
        if results:
            display_arxiv_results(results, summary_button)
        else:
            st.write("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    if st.session_state.axiv_id is not None:
        for arxiv_id in st.session_state.axiv_id:
            arxiv_data = load_arxiv(arxiv_id)
            save_paper_to_csv(arxiv_id, arxiv_data)


def handle_interesting_papers(df):
    selected_df = dataframe_with_selections(df)

    for i, (title, abstract, arxiv_id) in enumerate(
        selected_df[["Title", "Summary", "arxiv_id"]].values
    ):
        with st.container(border=True):
            st.markdown(f"### {i+1}. {title} abstract")
            st.markdown(abstract)
            col = st.columns([1, 2])
            with col[0]:
                trans_button = st.button("Translate abstract", key=f"trans_{i}")
            with col[1]:
                target_lang = st.selectbox(
                    "Target Language", ["ko", "en"], key=f"t_lang_{i}"
                )
            file_name = f"./data/paper_csv/{arxiv_id}_{target_lang}.json"
            translated_abstract = None
            if os.path.exists(file_name):
                with open(file_name, "r", encoding="utf-8-sig") as f:
                    translated_abstract = json.loads(f.read())["translated"]  # type: ignore
                st.expander("Translated Abstract").markdown(translated_abstract)

            else:
                if trans_button:
                    # load translated abstract
                    translated_abstract = translate(abstract, target_lang)  # type: ignore
                    st.expander("Translated Abstract", expanded=True).markdown(
                        translated_abstract
                    )

                    # save translated abstract as json
                    translated_abstract = {
                        "source": abstract,
                        "translated": translated_abstract,
                    }
                    with open(file_name, "w", encoding="UTF-8-sig") as f:
                        f.write(json.dumps(translated_abstract, ensure_ascii=False))

            if st.button("Search on Youtube", key=f"you_{i}", use_container_width=True):
                data = {
                    "title": title,
                    "arxiv_id": arxiv_id,
                }
                st.session_state.paper_data = data
                st.switch_page("./pages/ğŸ¥ YouTube Search.py")

            if st.button("Go to Review Page", key=f"re_{i}", use_container_width=True):
                # pass data
                data = {
                    "df": selected_df,
                    "translated_abstract": translated_abstract,
                }
                st.session_state.paper_data = data
                st.switch_page("./pages/ğŸ“„ Review page.py")


if "axiv_id" not in st.session_state:
    st.session_state.axiv_id = []

st.set_page_config(
    page_title="Review Paper Home",
    page_icon="ğŸ ",
)

st.markdown(
    """
# ë…¼ë¬¸ ë¦¬ë·° í˜ì´ì§€

> ì´ í˜ì´ì§€ëŠ” ìœ íŠœë¸Œ ë¦¬ë·° ì˜ìƒê³¼ Arxiv ë…¼ë¬¸ì„ í†µí•©í•˜ì—¬ ìµœì‹  ì—°êµ¬ ë‚´ìš©ì„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦½ë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **ìœ íŠœë¸Œ ë¦¬ë·° ìŒì„± ë³€í™˜**: ìœ íŠœë¸Œ ë¦¬ë·° ì˜ìƒì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤.
- **ë…¼ë¬¸ ìš”ì•½ ì œê³µ**: Arxiv ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ ì œê³µí•©ë‹ˆë‹¤.
- **AI ê¸°ë°˜ Q&A**: ë…¼ë¬¸ê³¼ ë¦¬ë·°ì— ëŒ€í•œ ì§ˆë¬¸ì— AIê°€ ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ë…¼ë¬¸ ë‚´ìš©ì„ ë¹ ë¥´ê²Œ ì´í•´í•˜ê³ , ê¶ê¸ˆí•œ ì ì„ í•´ê²°í•˜ì„¸ìš”.
"""
)

df = load_csv()
interest_paper_list = df["arxiv_id"].values.tolist()
st.session_state.interest_paper_list = interest_paper_list

with st.container(border=True):
    st.markdown("## Arxiv Search")

    # get user query for arxiv search
    query = st.text_input("Arxivì—ì„œ ê²€ìƒ‰í•  ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    handle_arxiv_search(query)

with st.container(border=True):
    st.markdown("## Interesting Papers")
    handle_interesting_papers(df)
