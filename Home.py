import streamlit as st
import pandas as pd
import os
import json
from src.arxiv_search import search_arxiv, display_arxiv_results
from src.translator import translate
from src.utils import load_csv
from langchain_community.document_loaders import ArxivLoader


def load_arxiv(arxiv_id, with_meta=True):
    try:
        # arxiv_id í˜•ì‹ í‘œì¤€í™”
        # ê°€ë” 'v1'ê³¼ ê°™ì€ ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
        if "v" in arxiv_id:
            arxiv_id = arxiv_id.split("v")[0]

        loader = ArxivLoader(arxiv_id, load_all_available_meta=with_meta)
        return loader.load()
    except Exception as e:
        st.error(f"ë…¼ë¬¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        st.warning(
            f"ID '{arxiv_id}'ë¡œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìœ íš¨í•œ ArXiv IDì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        )
        return []


def dataframe_with_selections(df):
    df_with_selections = df.copy()
    df_with_selections.insert(0, "Select", False)

    edited_df = st.data_editor(
        df_with_selections,
        hide_index=True,
        column_config={"Select": st.column_config.CheckboxColumn(required=True)},
        disabled=df.columns,
    )

    selected_rows = edited_df[edited_df.Select]

    return selected_rows.drop(columns=["Select"])


def save_paper_to_csv(arxiv_id, arxiv_data):
    try:
        # ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬ ì¤‘ë‹¨
        if not arxiv_data or len(arxiv_data) == 0:
            st.warning(f"ì €ì¥í•  ë…¼ë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {arxiv_id}")
            return

        # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
        os.makedirs("./data/paper_csv", exist_ok=True)

        if os.path.exists(f"./data/paper_csv/paper.csv"):
            df = pd.read_csv(f"./data/paper_csv/paper.csv")

            if arxiv_id in df["arxiv_id"].values:
                st.write("ì´ë¯¸ ì €ì¥ëœ ë…¼ë¬¸ì…ë‹ˆë‹¤.")
                return
            else:
                new_df = pd.json_normalize(arxiv_data[0].dict()["metadata"])
                new_df["arxiv_id"] = arxiv_id
                df = pd.concat([df, new_df])
                df.to_csv(f"./data/paper_csv/paper.csv", index=False)
        else:
            new_df = pd.json_normalize(arxiv_data[0].dict()["metadata"])
            new_df["arxiv_id"] = arxiv_id
            new_df.to_csv(f"./data/paper_csv/paper.csv", index=False)

        st.success(f"ë…¼ë¬¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {arxiv_id}")
    except Exception as e:
        st.error(f"ë…¼ë¬¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


def handle_arxiv_search(query):
    results = search_arxiv(query)
    summary_button = st.checkbox("Full Summary", False)

    if st.button("ê²€ìƒ‰"):
        if results:
            display_arxiv_results(results, summary_button)
        else:
            st.write("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    if st.session_state.axiv_id is not None and len(st.session_state.axiv_id) > 0:
        arxiv_ids_to_remove = []
        for arxiv_id in st.session_state.axiv_id:
            arxiv_data = load_arxiv(arxiv_id)
            if arxiv_data:
                save_paper_to_csv(arxiv_id, arxiv_data)
            else:
                # ë…¼ë¬¸ ë¡œë“œì— ì‹¤íŒ¨í•œ ê²½ìš° ID ëª©ë¡ì—ì„œ ì œê±°
                arxiv_ids_to_remove.append(arxiv_id)

        # ì‹¤íŒ¨í•œ ID ì œê±°
        for arxiv_id in arxiv_ids_to_remove:
            if arxiv_id in st.session_state.axiv_id:
                st.session_state.axiv_id.remove(arxiv_id)


def handle_interesting_papers(df):
    selected_df = dataframe_with_selections(df)

    for i, (title, abstract, arxiv_id) in enumerate(
        selected_df[["Title", "Summary", "arxiv_id"]].values
    ):
        with st.container(border=True):
            st.markdown(f"### {i+1}. {title} abstract")
            st.markdown(abstract)
            col = st.columns([1, 2])
            with col[0]:
                trans_button = st.button("Translate abstract", key=f"trans_{i}")
            with col[1]:
                target_lang = st.selectbox(
                    "Target Language", ["ko", "en"], key=f"t_lang_{i}"
                )
            file_name = f"./data/paper_csv/{arxiv_id}_{target_lang}.json"
            translated_abstract = None
            if os.path.exists(file_name):
                with open(file_name, "r", encoding="utf-8-sig") as f:
                    translated_abstract = json.loads(f.read())["translated"]  # type: ignore
                st.expander("Translated Abstract").markdown(translated_abstract)

            else:
                if trans_button:
                    # load translated abstract
                    translated_abstract = translate(abstract, target_lang)  # type: ignore
                    st.expander("Translated Abstract", expanded=True).markdown(
                        translated_abstract
                    )

                    # save translated abstract as json
                    translated_abstract = {
                        "source": abstract,
                        "translated": translated_abstract,
                    }
                    with open(file_name, "w", encoding="UTF-8-sig") as f:
                        f.write(json.dumps(translated_abstract, ensure_ascii=False))

            if st.button("Search on Youtube", key=f"you_{i}", use_container_width=True):
                data = {
                    "title": title,
                    "arxiv_id": arxiv_id,
                }
                st.session_state.paper_data = data
                st.switch_page("./pages/ğŸ¥ YouTube Search.py")

            if st.button("Go to Review Page", key=f"re_{i}", use_container_width=True):
                # pass data
                data = {
                    "df": selected_df,
                    "translated_abstract": translated_abstract,
                }
                st.session_state.paper_data = data
                st.switch_page("./pages/ğŸ“„ Review page.py")


if "axiv_id" not in st.session_state:
    st.session_state.axiv_id = []

if "paper_data" not in st.session_state:
    st.session_state.paper_data = None

st.set_page_config(
    page_title="Review Paper Home",
    page_icon="ğŸ ",
)

st.markdown(
    """
# ë…¼ë¬¸ ë¦¬ë·° í˜ì´ì§€

> ì´ í˜ì´ì§€ëŠ” ìœ íŠœë¸Œ ë¦¬ë·° ì˜ìƒê³¼ Arxiv ë…¼ë¬¸ì„ í†µí•©í•˜ì—¬ ìµœì‹  ì—°êµ¬ ë‚´ìš©ì„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦½ë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **ìœ íŠœë¸Œ ë¦¬ë·° ìŒì„± ë³€í™˜**: ìœ íŠœë¸Œ ë¦¬ë·° ì˜ìƒì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤.
- **ë…¼ë¬¸ ìš”ì•½ ì œê³µ**: Arxiv ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ ì œê³µí•©ë‹ˆë‹¤.
- **AI ê¸°ë°˜ Q&A**: ë…¼ë¬¸ê³¼ ë¦¬ë·°ì— ëŒ€í•œ ì§ˆë¬¸ì— AIê°€ ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ë…¼ë¬¸ ë‚´ìš©ì„ ë¹ ë¥´ê²Œ ì´í•´í•˜ê³ , ê¶ê¸ˆí•œ ì ì„ í•´ê²°í•˜ì„¸ìš”.
"""
)

df = load_csv()
interest_paper_list = df["arxiv_id"].values.tolist()
st.session_state.interest_paper_list = interest_paper_list

with st.container(border=True):
    st.markdown("## Arxiv Search")

    # get user query for arxiv search
    query = st.text_input("Arxivì—ì„œ ê²€ìƒ‰í•  ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    handle_arxiv_search(query)

with st.container(border=True):
    st.markdown("## Interesting Papers")
    handle_interesting_papers(df)
